{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ccdd29f-4ad5-476f-becf-d5d74af5a7ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting PyMuPDF\n",
      "  Downloading PyMuPDF-1.24.7-cp312-none-win_amd64.whl.metadata (3.4 kB)\n",
      "Collecting PyMuPDFb==1.24.6 (from PyMuPDF)\n",
      "  Downloading PyMuPDFb-1.24.6-py3-none-win_amd64.whl.metadata (1.4 kB)\n",
      "Downloading PyMuPDF-1.24.7-cp312-none-win_amd64.whl (3.2 MB)\n",
      "   ---------------------------------------- 0.0/3.2 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/3.2 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/3.2 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/3.2 MB 435.7 kB/s eta 0:00:08\n",
      "   - -------------------------------------- 0.1/3.2 MB 980.4 kB/s eta 0:00:04\n",
      "   -- ------------------------------------- 0.2/3.2 MB 1.1 MB/s eta 0:00:03\n",
      "   -- ------------------------------------- 0.2/3.2 MB 1.3 MB/s eta 0:00:03\n",
      "   --- ------------------------------------ 0.3/3.2 MB 1.3 MB/s eta 0:00:03\n",
      "   ---- ----------------------------------- 0.4/3.2 MB 1.4 MB/s eta 0:00:03\n",
      "   ----- ---------------------------------- 0.4/3.2 MB 1.4 MB/s eta 0:00:02\n",
      "   ------ --------------------------------- 0.5/3.2 MB 1.3 MB/s eta 0:00:03\n",
      "   ------ --------------------------------- 0.5/3.2 MB 1.4 MB/s eta 0:00:02\n",
      "   ------- -------------------------------- 0.6/3.2 MB 1.4 MB/s eta 0:00:02\n",
      "   -------- ------------------------------- 0.7/3.2 MB 1.4 MB/s eta 0:00:02\n",
      "   --------- ------------------------------ 0.7/3.2 MB 1.4 MB/s eta 0:00:02\n",
      "   ---------- ----------------------------- 0.8/3.2 MB 1.4 MB/s eta 0:00:02\n",
      "   ----------- ---------------------------- 0.9/3.2 MB 1.4 MB/s eta 0:00:02\n",
      "   ------------ --------------------------- 1.0/3.2 MB 1.4 MB/s eta 0:00:02\n",
      "   ------------ --------------------------- 1.0/3.2 MB 1.4 MB/s eta 0:00:02\n",
      "   ------------- -------------------------- 1.1/3.2 MB 1.4 MB/s eta 0:00:02\n",
      "   -------------- ------------------------- 1.2/3.2 MB 1.4 MB/s eta 0:00:02\n",
      "   --------------- ------------------------ 1.2/3.2 MB 1.4 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 1.3/3.2 MB 1.4 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 1.4/3.2 MB 1.4 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 1.4/3.2 MB 1.5 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 1.4/3.2 MB 1.4 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 1.6/3.2 MB 1.5 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 1.6/3.2 MB 1.4 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 1.7/3.2 MB 1.4 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 1.8/3.2 MB 1.5 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 1.8/3.2 MB 1.5 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 1.9/3.2 MB 1.5 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 1.9/3.2 MB 1.5 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 1.9/3.2 MB 1.5 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 1.9/3.2 MB 1.5 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 2.1/3.2 MB 1.4 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 2.2/3.2 MB 1.5 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 2.3/3.2 MB 1.5 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 2.4/3.2 MB 1.5 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 2.4/3.2 MB 1.5 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 2.5/3.2 MB 1.5 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 2.6/3.2 MB 1.5 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 2.6/3.2 MB 1.5 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 2.7/3.2 MB 1.5 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 2.8/3.2 MB 1.5 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 2.8/3.2 MB 1.5 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 2.9/3.2 MB 1.5 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 2.9/3.2 MB 1.5 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 3.0/3.2 MB 1.5 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 3.1/3.2 MB 1.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  3.2/3.2 MB 1.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 3.2/3.2 MB 1.5 MB/s eta 0:00:00\n",
      "Downloading PyMuPDFb-1.24.6-py3-none-win_amd64.whl (12.5 MB)\n",
      "   ---------------------------------------- 0.0/12.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.1/12.5 MB 2.2 MB/s eta 0:00:06\n",
      "    --------------------------------------- 0.2/12.5 MB 1.6 MB/s eta 0:00:08\n",
      "    --------------------------------------- 0.2/12.5 MB 1.7 MB/s eta 0:00:08\n",
      "    --------------------------------------- 0.3/12.5 MB 1.7 MB/s eta 0:00:08\n",
      "   - -------------------------------------- 0.4/12.5 MB 1.6 MB/s eta 0:00:08\n",
      "   - -------------------------------------- 0.4/12.5 MB 1.5 MB/s eta 0:00:08\n",
      "   - -------------------------------------- 0.5/12.5 MB 1.6 MB/s eta 0:00:08\n",
      "   - -------------------------------------- 0.6/12.5 MB 1.6 MB/s eta 0:00:08\n",
      "   -- ------------------------------------- 0.6/12.5 MB 1.5 MB/s eta 0:00:08\n",
      "   -- ------------------------------------- 0.7/12.5 MB 1.6 MB/s eta 0:00:08\n",
      "   -- ------------------------------------- 0.8/12.5 MB 1.5 MB/s eta 0:00:08\n",
      "   -- ------------------------------------- 0.8/12.5 MB 1.5 MB/s eta 0:00:08\n",
      "   -- ------------------------------------- 0.9/12.5 MB 1.5 MB/s eta 0:00:08\n",
      "   --- ------------------------------------ 1.0/12.5 MB 1.5 MB/s eta 0:00:08\n",
      "   --- ------------------------------------ 1.1/12.5 MB 1.5 MB/s eta 0:00:08\n",
      "   --- ------------------------------------ 1.1/12.5 MB 1.5 MB/s eta 0:00:08\n",
      "   --- ------------------------------------ 1.2/12.5 MB 1.5 MB/s eta 0:00:08\n",
      "   --- ------------------------------------ 1.2/12.5 MB 1.5 MB/s eta 0:00:08\n",
      "   ---- ----------------------------------- 1.3/12.5 MB 1.5 MB/s eta 0:00:08\n",
      "   ---- ----------------------------------- 1.4/12.5 MB 1.5 MB/s eta 0:00:08\n",
      "   ---- ----------------------------------- 1.5/12.5 MB 1.5 MB/s eta 0:00:08\n",
      "   ---- ----------------------------------- 1.5/12.5 MB 1.5 MB/s eta 0:00:08\n",
      "   ----- ---------------------------------- 1.6/12.5 MB 1.5 MB/s eta 0:00:08\n",
      "   ----- ---------------------------------- 1.6/12.5 MB 1.5 MB/s eta 0:00:08\n",
      "   ----- ---------------------------------- 1.7/12.5 MB 1.5 MB/s eta 0:00:08\n",
      "   ----- ---------------------------------- 1.8/12.5 MB 1.5 MB/s eta 0:00:08\n",
      "   ----- ---------------------------------- 1.8/12.5 MB 1.5 MB/s eta 0:00:08\n",
      "   ------ --------------------------------- 1.9/12.5 MB 1.5 MB/s eta 0:00:07\n",
      "   ------ --------------------------------- 1.9/12.5 MB 1.5 MB/s eta 0:00:07\n",
      "   ------ --------------------------------- 2.0/12.5 MB 1.5 MB/s eta 0:00:07\n",
      "   ------ --------------------------------- 2.1/12.5 MB 1.5 MB/s eta 0:00:07\n",
      "   ------ --------------------------------- 2.1/12.5 MB 1.5 MB/s eta 0:00:07\n",
      "   ------- -------------------------------- 2.2/12.5 MB 1.5 MB/s eta 0:00:07\n",
      "   ------- -------------------------------- 2.3/12.5 MB 1.5 MB/s eta 0:00:07\n",
      "   ------- -------------------------------- 2.3/12.5 MB 1.5 MB/s eta 0:00:07\n",
      "   ------- -------------------------------- 2.4/12.5 MB 1.5 MB/s eta 0:00:07\n",
      "   ------- -------------------------------- 2.4/12.5 MB 1.5 MB/s eta 0:00:07\n",
      "   ------- -------------------------------- 2.4/12.5 MB 1.5 MB/s eta 0:00:07\n",
      "   ------- -------------------------------- 2.4/12.5 MB 1.5 MB/s eta 0:00:07\n",
      "   -------- ------------------------------- 2.6/12.5 MB 1.5 MB/s eta 0:00:07\n",
      "   -------- ------------------------------- 2.6/12.5 MB 1.5 MB/s eta 0:00:07\n",
      "   -------- ------------------------------- 2.7/12.5 MB 1.5 MB/s eta 0:00:07\n",
      "   -------- ------------------------------- 2.8/12.5 MB 1.5 MB/s eta 0:00:07\n",
      "   --------- ------------------------------ 2.9/12.5 MB 1.5 MB/s eta 0:00:07\n",
      "   --------- ------------------------------ 2.9/12.5 MB 1.5 MB/s eta 0:00:07\n",
      "   --------- ------------------------------ 3.0/12.5 MB 1.5 MB/s eta 0:00:07\n",
      "   --------- ------------------------------ 3.1/12.5 MB 1.5 MB/s eta 0:00:07\n",
      "   ---------- ----------------------------- 3.2/12.5 MB 1.5 MB/s eta 0:00:07\n",
      "   ---------- ----------------------------- 3.2/12.5 MB 1.5 MB/s eta 0:00:07\n",
      "   ---------- ----------------------------- 3.3/12.5 MB 1.5 MB/s eta 0:00:07\n",
      "   ---------- ----------------------------- 3.4/12.5 MB 1.5 MB/s eta 0:00:07\n",
      "   ----------- ---------------------------- 3.5/12.5 MB 1.5 MB/s eta 0:00:06\n",
      "   ----------- ---------------------------- 3.6/12.5 MB 1.5 MB/s eta 0:00:06\n",
      "   ----------- ---------------------------- 3.7/12.5 MB 1.5 MB/s eta 0:00:06\n",
      "   ----------- ---------------------------- 3.7/12.5 MB 1.5 MB/s eta 0:00:06\n",
      "   ------------ --------------------------- 3.8/12.5 MB 1.5 MB/s eta 0:00:06\n",
      "   ------------ --------------------------- 3.8/12.5 MB 1.5 MB/s eta 0:00:06\n",
      "   ------------ --------------------------- 4.0/12.5 MB 1.5 MB/s eta 0:00:06\n",
      "   ------------ --------------------------- 4.0/12.5 MB 1.5 MB/s eta 0:00:06\n",
      "   ------------- -------------------------- 4.1/12.5 MB 1.5 MB/s eta 0:00:06\n",
      "   ------------- -------------------------- 4.2/12.5 MB 1.5 MB/s eta 0:00:06\n",
      "   ------------- -------------------------- 4.2/12.5 MB 1.5 MB/s eta 0:00:06\n",
      "   ------------- -------------------------- 4.3/12.5 MB 1.5 MB/s eta 0:00:06\n",
      "   -------------- ------------------------- 4.4/12.5 MB 1.5 MB/s eta 0:00:06\n",
      "   -------------- ------------------------- 4.4/12.5 MB 1.5 MB/s eta 0:00:06\n",
      "   -------------- ------------------------- 4.5/12.5 MB 1.5 MB/s eta 0:00:06\n",
      "   -------------- ------------------------- 4.6/12.5 MB 1.5 MB/s eta 0:00:06\n",
      "   --------------- ------------------------ 4.7/12.5 MB 1.5 MB/s eta 0:00:06\n",
      "   --------------- ------------------------ 4.8/12.5 MB 1.5 MB/s eta 0:00:06\n",
      "   --------------- ------------------------ 4.9/12.5 MB 1.5 MB/s eta 0:00:06\n",
      "   --------------- ------------------------ 4.9/12.5 MB 1.5 MB/s eta 0:00:06\n",
      "   --------------- ------------------------ 5.0/12.5 MB 1.5 MB/s eta 0:00:05\n",
      "   ---------------- ----------------------- 5.0/12.5 MB 1.5 MB/s eta 0:00:05\n",
      "   ---------------- ----------------------- 5.1/12.5 MB 1.5 MB/s eta 0:00:05\n",
      "   ---------------- ----------------------- 5.2/12.5 MB 1.5 MB/s eta 0:00:05\n",
      "   ---------------- ----------------------- 5.2/12.5 MB 1.5 MB/s eta 0:00:05\n",
      "   ---------------- ----------------------- 5.3/12.5 MB 1.5 MB/s eta 0:00:05\n",
      "   ----------------- ---------------------- 5.3/12.5 MB 1.5 MB/s eta 0:00:05\n",
      "   ----------------- ---------------------- 5.4/12.5 MB 1.5 MB/s eta 0:00:05\n",
      "   ----------------- ---------------------- 5.5/12.5 MB 1.5 MB/s eta 0:00:05\n",
      "   ----------------- ---------------------- 5.5/12.5 MB 1.5 MB/s eta 0:00:05\n",
      "   ----------------- ---------------------- 5.5/12.5 MB 1.5 MB/s eta 0:00:05\n",
      "   ------------------ --------------------- 5.7/12.5 MB 1.5 MB/s eta 0:00:05\n",
      "   ------------------ --------------------- 5.8/12.5 MB 1.5 MB/s eta 0:00:05\n",
      "   ------------------ --------------------- 5.8/12.5 MB 1.5 MB/s eta 0:00:05\n",
      "   ------------------- -------------------- 5.9/12.5 MB 1.5 MB/s eta 0:00:05\n",
      "   ------------------- -------------------- 6.0/12.5 MB 1.5 MB/s eta 0:00:05\n",
      "   ------------------- -------------------- 6.1/12.5 MB 1.5 MB/s eta 0:00:05\n",
      "   ------------------- -------------------- 6.1/12.5 MB 1.5 MB/s eta 0:00:05\n",
      "   ------------------- -------------------- 6.2/12.5 MB 1.5 MB/s eta 0:00:05\n",
      "   -------------------- ------------------- 6.2/12.5 MB 1.5 MB/s eta 0:00:05\n",
      "   -------------------- ------------------- 6.3/12.5 MB 1.5 MB/s eta 0:00:05\n",
      "   -------------------- ------------------- 6.4/12.5 MB 1.5 MB/s eta 0:00:05\n",
      "   -------------------- ------------------- 6.4/12.5 MB 1.5 MB/s eta 0:00:05\n",
      "   -------------------- ------------------- 6.5/12.5 MB 1.5 MB/s eta 0:00:04\n",
      "   --------------------- ------------------ 6.6/12.5 MB 1.5 MB/s eta 0:00:04\n",
      "   --------------------- ------------------ 6.6/12.5 MB 1.5 MB/s eta 0:00:04\n",
      "   --------------------- ------------------ 6.7/12.5 MB 1.5 MB/s eta 0:00:04\n",
      "   --------------------- ------------------ 6.8/12.5 MB 1.5 MB/s eta 0:00:04\n",
      "   --------------------- ------------------ 6.8/12.5 MB 1.5 MB/s eta 0:00:04\n",
      "   ---------------------- ----------------- 6.9/12.5 MB 1.5 MB/s eta 0:00:04\n",
      "   ---------------------- ----------------- 6.9/12.5 MB 1.5 MB/s eta 0:00:04\n",
      "   ---------------------- ----------------- 7.0/12.5 MB 1.5 MB/s eta 0:00:04\n",
      "   ---------------------- ----------------- 7.1/12.5 MB 1.5 MB/s eta 0:00:04\n",
      "   ---------------------- ----------------- 7.1/12.5 MB 1.5 MB/s eta 0:00:04\n",
      "   ----------------------- ---------------- 7.2/12.5 MB 1.5 MB/s eta 0:00:04\n",
      "   ----------------------- ---------------- 7.2/12.5 MB 1.5 MB/s eta 0:00:04\n",
      "   ----------------------- ---------------- 7.3/12.5 MB 1.5 MB/s eta 0:00:04\n",
      "   ----------------------- ---------------- 7.4/12.5 MB 1.5 MB/s eta 0:00:04\n",
      "   ----------------------- ---------------- 7.4/12.5 MB 1.5 MB/s eta 0:00:04\n",
      "   ------------------------ --------------- 7.5/12.5 MB 1.5 MB/s eta 0:00:04\n",
      "   ------------------------ --------------- 7.6/12.5 MB 1.5 MB/s eta 0:00:04\n",
      "   ------------------------ --------------- 7.7/12.5 MB 1.5 MB/s eta 0:00:04\n",
      "   ------------------------ --------------- 7.7/12.5 MB 1.5 MB/s eta 0:00:04\n",
      "   ------------------------ --------------- 7.8/12.5 MB 1.5 MB/s eta 0:00:04\n",
      "   ------------------------- -------------- 7.9/12.5 MB 1.5 MB/s eta 0:00:04\n",
      "   ------------------------- -------------- 7.9/12.5 MB 1.5 MB/s eta 0:00:04\n",
      "   ------------------------- -------------- 8.0/12.5 MB 1.5 MB/s eta 0:00:03\n",
      "   ------------------------- -------------- 8.0/12.5 MB 1.5 MB/s eta 0:00:03\n",
      "   -------------------------- ------------- 8.1/12.5 MB 1.5 MB/s eta 0:00:03\n",
      "   -------------------------- ------------- 8.2/12.5 MB 1.5 MB/s eta 0:00:03\n",
      "   -------------------------- ------------- 8.2/12.5 MB 1.5 MB/s eta 0:00:03\n",
      "   -------------------------- ------------- 8.3/12.5 MB 1.5 MB/s eta 0:00:03\n",
      "   -------------------------- ------------- 8.3/12.5 MB 1.5 MB/s eta 0:00:03\n",
      "   -------------------------- ------------- 8.4/12.5 MB 1.5 MB/s eta 0:00:03\n",
      "   --------------------------- ------------ 8.5/12.5 MB 1.5 MB/s eta 0:00:03\n",
      "   --------------------------- ------------ 8.6/12.5 MB 1.5 MB/s eta 0:00:03\n",
      "   --------------------------- ------------ 8.6/12.5 MB 1.5 MB/s eta 0:00:03\n",
      "   --------------------------- ------------ 8.7/12.5 MB 1.5 MB/s eta 0:00:03\n",
      "   ---------------------------- ----------- 8.8/12.5 MB 1.5 MB/s eta 0:00:03\n",
      "   ---------------------------- ----------- 8.8/12.5 MB 1.5 MB/s eta 0:00:03\n",
      "   ---------------------------- ----------- 8.9/12.5 MB 1.5 MB/s eta 0:00:03\n",
      "   ---------------------------- ----------- 9.0/12.5 MB 1.5 MB/s eta 0:00:03\n",
      "   ----------------------------- ---------- 9.0/12.5 MB 1.5 MB/s eta 0:00:03\n",
      "   ----------------------------- ---------- 9.1/12.5 MB 1.5 MB/s eta 0:00:03\n",
      "   ----------------------------- ---------- 9.1/12.5 MB 1.5 MB/s eta 0:00:03\n",
      "   ----------------------------- ---------- 9.2/12.5 MB 1.5 MB/s eta 0:00:03\n",
      "   ----------------------------- ---------- 9.3/12.5 MB 1.5 MB/s eta 0:00:03\n",
      "   ------------------------------ --------- 9.4/12.5 MB 1.5 MB/s eta 0:00:03\n",
      "   ------------------------------ --------- 9.4/12.5 MB 1.5 MB/s eta 0:00:03\n",
      "   ------------------------------ --------- 9.5/12.5 MB 1.5 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 9.5/12.5 MB 1.5 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 9.6/12.5 MB 1.5 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 9.7/12.5 MB 1.5 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 9.7/12.5 MB 1.5 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 9.8/12.5 MB 1.5 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 9.9/12.5 MB 1.5 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 10.0/12.5 MB 1.5 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 10.1/12.5 MB 1.5 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 10.1/12.5 MB 1.5 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 10.2/12.5 MB 1.5 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 10.3/12.5 MB 1.5 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 10.4/12.5 MB 1.5 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 10.4/12.5 MB 1.5 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 10.5/12.5 MB 1.5 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 10.5/12.5 MB 1.5 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 10.6/12.5 MB 1.5 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 10.7/12.5 MB 1.5 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 10.8/12.5 MB 1.5 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 10.8/12.5 MB 1.5 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 10.9/12.5 MB 1.5 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 10.9/12.5 MB 1.5 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 11.0/12.5 MB 1.5 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 11.1/12.5 MB 1.5 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 11.2/12.5 MB 1.5 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 11.2/12.5 MB 1.5 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 11.3/12.5 MB 1.5 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 11.3/12.5 MB 1.5 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 11.4/12.5 MB 1.5 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 11.5/12.5 MB 1.5 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 11.5/12.5 MB 1.5 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 11.6/12.5 MB 1.5 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 11.7/12.5 MB 1.5 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 11.7/12.5 MB 1.5 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 11.8/12.5 MB 1.5 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 11.8/12.5 MB 1.5 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 11.9/12.5 MB 1.5 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 12.0/12.5 MB 1.5 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 12.0/12.5 MB 1.5 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 12.1/12.5 MB 1.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  12.2/12.5 MB 1.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  12.2/12.5 MB 1.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  12.3/12.5 MB 1.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  12.4/12.5 MB 1.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  12.4/12.5 MB 1.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  12.5/12.5 MB 1.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  12.5/12.5 MB 1.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 12.5/12.5 MB 1.5 MB/s eta 0:00:00\n",
      "Installing collected packages: PyMuPDFb, PyMuPDF\n",
      "Successfully installed PyMuPDF-1.24.7 PyMuPDFb-1.24.6\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install PyMuPDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "09f8d350-64aa-4eab-987d-306151919abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz\n",
    "import re\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "764181a8-0701-4bdb-8202-790385d2ddfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_pdf(file_path):\n",
    "    doc = fitz.open(file_path)\n",
    "    text = \"\"\n",
    "    for page in doc:\n",
    "        text += page.get_text()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2e6c70b4-74a8-4b57-afcf-50a24eed56f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path=r'E:\\new_downloads_default_folder\\RESUME_AS_OF_17_JUNE.pdf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6486fdde-d11b-4956-9ed1-bb91fa63316b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n  \\n \\n1 Speech to Speech Chatbot for Airplane flying school  |   LINK \\n \\n\\uf0be Performed Text Processing to create Question Answer pair Template  \\n\\uf0be Used these Tools and technologies Transformers, Llama-2-7b, NLP, PyTorch, Hugging \\nFace, LoRA, QLoRA, Parameter efficient Fine Tuning(PEFT), Supervised Fine \\nTuning(SFT) \\n\\uf0be Then Re-trained the Llama-2-7b model using hugging face API and dataset I took was,  \\nAirplane Flying Handbook of US Federal Aviation Administration(DOT-FAA). \\n \\n\\uf0be Took prompts in speech medium and then used Speech to text technology to convert speech \\ninto text and then passed the text to our customized chatbot. The chat gave response in text \\nform which I converted it into speech medium using Text to Speech technology. \\n. \\n \\n2  Object detection model  |  LINK \\n\\uf0be Used Transfer Learning to get pretrained YOLOv8 model from huggingface. \\n\\uf0be Improvements done:  \\n1. Added audio to the output which is not present in output of yolov8. \\n2. Calculated distance between objects in the video frame. \\n3. Created circular bounding box instead of rectangular bounding box. \\nKUNAL KUMAR \\nDATA SCIENTIST \\n \\n \\nPhone No: 9060804108                 \\n \\nEmail: kunal.jcdu@gmail.com    \\n \\nLinkedin | ( Kunal Kumar)              \\n \\nGithub | (SolveKunal)                        \\n \\n SKILLS  SUMMARY \\n   WORK EXPERIENCE \\n \\n \\n \\n \\n \\n \\nDATA ANALYST INTERN  |   LINK \\n\\uf0b7 Streamlined Data collection and reporting procedures, reducing processing time by 10%,   \\nenhancing efficiency. \\n\\uf0b7 Collaborated  with 3+ cross-functional teams to gather information to decide Project \\nscopes and ensure alignment with business objectives. \\n\\uf0b7 Conducted in-depth market research and analysis resulting identification of 5+ key trends \\nand created Dashboard using power BI \\n   PROJECTS \\n \\nLanguages:              Python, SQL \\nFrameworks:            Matplotlib, Seaborn, NLTK, Pandas, Numpy, Sicklet-Learn,  \\nTools:                Power BI, Tableau, MySQL, Azure, Visual studio code, Jupyter Notebook,   .        \\n.                                 Speech to Text, Text to Speech, pyttsx3, Speech Recognizer                         \\nTechnical Skills:  NLP, LLMs, GenAI solution using Azure OpenAI, Computer Vision, Object        \\ndetection, Image Classification, similar sentence search, article summarizer, Machine Learning, \\nsupervised Learning, Linear Regression , Logistic Regression, Random Forest, Decision Tree, Data \\nModelling, Data Cleaning, Exploratory Data Analysis, Data Visualization, Predictive Analytics, Market \\nAnalysis, Sentiment Analysis, Semantic Search, Transfer Learning, TensorFlow, Keras, Trend Analysis \\n \\n 3   Pneumonia Detection    LINK  \\n\\uf0be Built a CNN network to classify chest X-ray images as pneumonia-affected or normal. \\n\\uf0be Achieved 75.66% accuracy. Plan to improve accuracy by optimizing image size and model \\nparameters. \\n\\uf0be Used these Tools and Technology: CNN, Deep Learning, Transfer Learning, Keras \\n           Tensorflow, VGG-16 \\n \\n 4   Live Dashboard creation \\n\\uf0be Performed Data Cleaning and Exploratory data Analysis in python and then \\ntransferred the cleaned dataset to Power BI. \\n\\uf0be Identified 10+ major trends from the dataset which contributed to decision-making \\nprocess. \\n\\uf0be Interactive  Dashboard was built and the link was sent to the University Professor. \\n \\n5  Sentiment Analysis of 50000 Movie Reviews and Similar Sentence Search \\n\\uf0be Performed Text Processing and extracted Embeddings for each word using Word2Vec and \\nTransformers model.  \\n\\uf0be Then trained a Random-Forest Classifier with  Embeddings as inputs to predict Sentiment of \\neach movie review and do Similar Sentence Search in a supervised technique mode. \\n\\uf0be Using Transformer model I achieved  more similar sentence  searches as here \\nContext Aware Embeddings were used. \\n \\n6  Restaurant Revenue Prediction \\n\\uf0be This project was part of the hackathon organized by Great Learning platform.  \\n\\uf0be Performed Data Cleaning and Exploratory Data Analysis and then built a Random \\nForest Regressor ML model \\n\\uf0be I secured 55 Rank in this Hackathon, with 85.5% accuracy.  \\n\\uf0be However, after the competition was over, I increased the models accuracy to 95.5% \\nusing XG-Boost model. \\n \\n \\n\\uf0d8 Post Graduation in Artificial Intelligence and Machine Learning               Undergoing \\n           University of Texas at Austin \\n \\n\\uf0d8 B-Tech in Electronics and Communication  Engineering                             2014-2018 \\n           Vellore Institute of Technology (VIT Vellore) \\n   EDUCATION \\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_extracted_from_pdf=read_pdf(file_path)\n",
    "text_extracted_from_pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ceef3e5c-1de7-4f17-bf45-ca7471b13830",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2a35063e-82eb-4d75-a448-db5d5a9b8074",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' 1 Speech to Speech Chatbot for Airplane flying school | LINK \\uf0be Performed Text Processing to create Question Answer pair Template \\uf0be Used these Tools and technologies Transformers, Llama-2-7b, NLP, PyTorch, Hugging Face, LoRA, QLoRA, Parameter efficient Fine Tuning(PEFT), Supervised Fine Tuning(SFT) \\uf0be Then Re-trained the Llama-2-7b model using hugging face API and dataset I took was, Airplane Flying Handbook of US Federal Aviation Administration(DOT-FAA). \\uf0be Took prompts in speech medium and then used Speech to text technology to convert speech into text and then passed the text to our customized chatbot. The chat gave response in text form which I converted it into speech medium using Text to Speech technology. . 2 Object detection model | LINK \\uf0be Used Transfer Learning to get pretrained YOLOv8 model from huggingface. \\uf0be Improvements done: 1. Added audio to the output which is not present in output of yolov8. 2. Calculated distance between objects in the video frame. 3. Created circular bounding box instead of rectangular bounding box. KUNAL KUMAR DATA SCIENTIST Phone No: 9060804108 Email: kunal.jcdu@gmail.com Linkedin | ( Kunal Kumar) Github | (SolveKunal) SKILLS SUMMARY WORK EXPERIENCE DATA ANALYST INTERN | LINK \\uf0b7 Streamlined Data collection and reporting procedures, reducing processing time by 10%, enhancing efficiency. \\uf0b7 Collaborated with 3+ cross-functional teams to gather information to decide Project scopes and ensure alignment with business objectives. \\uf0b7 Conducted in-depth market research and analysis resulting identification of 5+ key trends and created Dashboard using power BI PROJECTS Languages: Python, SQL Frameworks: Matplotlib, Seaborn, NLTK, Pandas, Numpy, Sicklet-Learn, Tools: Power BI, Tableau, MySQL, Azure, Visual studio code, Jupyter Notebook, . . Speech to Text, Text to Speech, pyttsx3, Speech Recognizer Technical Skills: NLP, LLMs, GenAI solution using Azure OpenAI, Computer Vision, Object detection, Image Classification, similar sentence search, article summarizer, Machine Learning, supervised Learning, Linear Regression , Logistic Regression, Random Forest, Decision Tree, Data Modelling, Data Cleaning, Exploratory Data Analysis, Data Visualization, Predictive Analytics, Market Analysis, Sentiment Analysis, Semantic Search, Transfer Learning, TensorFlow, Keras, Trend Analysis 3 Pneumonia Detection LINK \\uf0be Built a CNN network to classify chest X-ray images as pneumonia-affected or normal. \\uf0be Achieved 75.66% accuracy. Plan to improve accuracy by optimizing image size and model parameters. \\uf0be Used these Tools and Technology: CNN, Deep Learning, Transfer Learning, Keras Tensorflow, VGG-16 4 Live Dashboard creation \\uf0be Performed Data Cleaning and Exploratory data Analysis in python and then transferred the cleaned dataset to Power BI. \\uf0be Identified 10+ major trends from the dataset which contributed to decision-making process. \\uf0be Interactive Dashboard was built and the link was sent to the University Professor. 5 Sentiment Analysis of 50000 Movie Reviews and Similar Sentence Search \\uf0be Performed Text Processing and extracted Embeddings for each word using Word2Vec and Transformers model. \\uf0be Then trained a Random-Forest Classifier with Embeddings as inputs to predict Sentiment of each movie review and do Similar Sentence Search in a supervised technique mode. \\uf0be Using Transformer model I achieved more similar sentence searches as here Context Aware Embeddings were used. 6 Restaurant Revenue Prediction \\uf0be This project was part of the hackathon organized by Great Learning platform. \\uf0be Performed Data Cleaning and Exploratory Data Analysis and then built a Random Forest Regressor ML model \\uf0be I secured 55 Rank in this Hackathon, with 85.5% accuracy. \\uf0be However, after the competition was over, I increased the models accuracy to 95.5% using XG-Boost model. \\uf0d8 Post Graduation in Artificial Intelligence and Machine Learning Undergoing University of Texas at Austin \\uf0d8 B-Tech in Electronics and Communication Engineering 2014-2018 Vellore Institute of Technology (VIT Vellore) EDUCATION '"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed_text=preprocess_text(text_extracted_from_pdf)\n",
    "preprocessed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2c152d93-ed93-48a7-9a69-b807c2657eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_name(text):\n",
    "    lines = text.split('\\n')\n",
    "    return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "21ed25fc-86dd-4dbc-924b-abae5fa8a76e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' 1 Speech to Speech Chatbot for Airplane flying school | LINK \\uf0be Performed Text Processing to create Question Answer pair Template \\uf0be Used these Tools and technologies Transformers, Llama-2-7b, NLP, PyTorch, Hugging Face, LoRA, QLoRA, Parameter efficient Fine Tuning(PEFT), Supervised Fine Tuning(SFT) \\uf0be Then Re-trained the Llama-2-7b model using hugging face API and dataset I took was, Airplane Flying Handbook of US Federal Aviation Administration(DOT-FAA). \\uf0be Took prompts in speech medium and then used Speech to text technology to convert speech into text and then passed the text to our customized chatbot. The chat gave response in text form which I converted it into speech medium using Text to Speech technology. . 2 Object detection model | LINK \\uf0be Used Transfer Learning to get pretrained YOLOv8 model from huggingface. \\uf0be Improvements done: 1. Added audio to the output which is not present in output of yolov8. 2. Calculated distance between objects in the video frame. 3. Created circular bounding box instead of rectangular bounding box. KUNAL KUMAR DATA SCIENTIST Phone No: 9060804108 Email: kunal.jcdu@gmail.com Linkedin | ( Kunal Kumar) Github | (SolveKunal) SKILLS SUMMARY WORK EXPERIENCE DATA ANALYST INTERN | LINK \\uf0b7 Streamlined Data collection and reporting procedures, reducing processing time by 10%, enhancing efficiency. \\uf0b7 Collaborated with 3+ cross-functional teams to gather information to decide Project scopes and ensure alignment with business objectives. \\uf0b7 Conducted in-depth market research and analysis resulting identification of 5+ key trends and created Dashboard using power BI PROJECTS Languages: Python, SQL Frameworks: Matplotlib, Seaborn, NLTK, Pandas, Numpy, Sicklet-Learn, Tools: Power BI, Tableau, MySQL, Azure, Visual studio code, Jupyter Notebook, . . Speech to Text, Text to Speech, pyttsx3, Speech Recognizer Technical Skills: NLP, LLMs, GenAI solution using Azure OpenAI, Computer Vision, Object detection, Image Classification, similar sentence search, article summarizer, Machine Learning, supervised Learning, Linear Regression , Logistic Regression, Random Forest, Decision Tree, Data Modelling, Data Cleaning, Exploratory Data Analysis, Data Visualization, Predictive Analytics, Market Analysis, Sentiment Analysis, Semantic Search, Transfer Learning, TensorFlow, Keras, Trend Analysis 3 Pneumonia Detection LINK \\uf0be Built a CNN network to classify chest X-ray images as pneumonia-affected or normal. \\uf0be Achieved 75.66% accuracy. Plan to improve accuracy by optimizing image size and model parameters. \\uf0be Used these Tools and Technology: CNN, Deep Learning, Transfer Learning, Keras Tensorflow, VGG-16 4 Live Dashboard creation \\uf0be Performed Data Cleaning and Exploratory data Analysis in python and then transferred the cleaned dataset to Power BI. \\uf0be Identified 10+ major trends from the dataset which contributed to decision-making process. \\uf0be Interactive Dashboard was built and the link was sent to the University Professor. 5 Sentiment Analysis of 50000 Movie Reviews and Similar Sentence Search \\uf0be Performed Text Processing and extracted Embeddings for each word using Word2Vec and Transformers model. \\uf0be Then trained a Random-Forest Classifier with Embeddings as inputs to predict Sentiment of each movie review and do Similar Sentence Search in a supervised technique mode. \\uf0be Using Transformer model I achieved more similar sentence searches as here Context Aware Embeddings were used. 6 Restaurant Revenue Prediction \\uf0be This project was part of the hackathon organized by Great Learning platform. \\uf0be Performed Data Cleaning and Exploratory Data Analysis and then built a Random Forest Regressor ML model \\uf0be I secured 55 Rank in this Hackathon, with 85.5% accuracy. \\uf0be However, after the competition was over, I increased the models accuracy to 95.5% using XG-Boost model. \\uf0d8 Post Graduation in Artificial Intelligence and Machine Learning Undergoing University of Texas at Austin \\uf0d8 B-Tech in Electronics and Communication Engineering 2014-2018 Vellore Institute of Technology (VIT Vellore) EDUCATION ']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_name(preprocessed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1f55029f-02d3-4778-8af9-452fae6a8db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_name2(text):\n",
    "    lines = text.split('\\n')\n",
    "    return lines[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f1af8cd6-f0e6-47e4-9a62-dca3e9469030",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' 1 Speech to Speech Chatbot for Airplane flying school | LINK \\uf0be Performed Text Processing to create Question Answer pair Template \\uf0be Used these Tools and technologies Transformers, Llama-2-7b, NLP, PyTorch, Hugging Face, LoRA, QLoRA, Parameter efficient Fine Tuning(PEFT), Supervised Fine Tuning(SFT) \\uf0be Then Re-trained the Llama-2-7b model using hugging face API and dataset I took was, Airplane Flying Handbook of US Federal Aviation Administration(DOT-FAA). \\uf0be Took prompts in speech medium and then used Speech to text technology to convert speech into text and then passed the text to our customized chatbot. The chat gave response in text form which I converted it into speech medium using Text to Speech technology. . 2 Object detection model | LINK \\uf0be Used Transfer Learning to get pretrained YOLOv8 model from huggingface. \\uf0be Improvements done: 1. Added audio to the output which is not present in output of yolov8. 2. Calculated distance between objects in the video frame. 3. Created circular bounding box instead of rectangular bounding box. KUNAL KUMAR DATA SCIENTIST Phone No: 9060804108 Email: kunal.jcdu@gmail.com Linkedin | ( Kunal Kumar) Github | (SolveKunal) SKILLS SUMMARY WORK EXPERIENCE DATA ANALYST INTERN | LINK \\uf0b7 Streamlined Data collection and reporting procedures, reducing processing time by 10%, enhancing efficiency. \\uf0b7 Collaborated with 3+ cross-functional teams to gather information to decide Project scopes and ensure alignment with business objectives. \\uf0b7 Conducted in-depth market research and analysis resulting identification of 5+ key trends and created Dashboard using power BI PROJECTS Languages: Python, SQL Frameworks: Matplotlib, Seaborn, NLTK, Pandas, Numpy, Sicklet-Learn, Tools: Power BI, Tableau, MySQL, Azure, Visual studio code, Jupyter Notebook, . . Speech to Text, Text to Speech, pyttsx3, Speech Recognizer Technical Skills: NLP, LLMs, GenAI solution using Azure OpenAI, Computer Vision, Object detection, Image Classification, similar sentence search, article summarizer, Machine Learning, supervised Learning, Linear Regression , Logistic Regression, Random Forest, Decision Tree, Data Modelling, Data Cleaning, Exploratory Data Analysis, Data Visualization, Predictive Analytics, Market Analysis, Sentiment Analysis, Semantic Search, Transfer Learning, TensorFlow, Keras, Trend Analysis 3 Pneumonia Detection LINK \\uf0be Built a CNN network to classify chest X-ray images as pneumonia-affected or normal. \\uf0be Achieved 75.66% accuracy. Plan to improve accuracy by optimizing image size and model parameters. \\uf0be Used these Tools and Technology: CNN, Deep Learning, Transfer Learning, Keras Tensorflow, VGG-16 4 Live Dashboard creation \\uf0be Performed Data Cleaning and Exploratory data Analysis in python and then transferred the cleaned dataset to Power BI. \\uf0be Identified 10+ major trends from the dataset which contributed to decision-making process. \\uf0be Interactive Dashboard was built and the link was sent to the University Professor. 5 Sentiment Analysis of 50000 Movie Reviews and Similar Sentence Search \\uf0be Performed Text Processing and extracted Embeddings for each word using Word2Vec and Transformers model. \\uf0be Then trained a Random-Forest Classifier with Embeddings as inputs to predict Sentiment of each movie review and do Similar Sentence Search in a supervised technique mode. \\uf0be Using Transformer model I achieved more similar sentence searches as here Context Aware Embeddings were used. 6 Restaurant Revenue Prediction \\uf0be This project was part of the hackathon organized by Great Learning platform. \\uf0be Performed Data Cleaning and Exploratory Data Analysis and then built a Random Forest Regressor ML model \\uf0be I secured 55 Rank in this Hackathon, with 85.5% accuracy. \\uf0be However, after the competition was over, I increased the models accuracy to 95.5% using XG-Boost model. \\uf0d8 Post Graduation in Artificial Intelligence and Machine Learning Undergoing University of Texas at Austin \\uf0d8 B-Tech in Electronics and Communication Engineering 2014-2018 Vellore Institute of Technology (VIT Vellore) EDUCATION '"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_name2(preprocessed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "744cec83-3f0f-4ba6-b416-13d02467a2da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_name3(text):\n",
    "    lines = text.split('\\n')\n",
    "    return lines[0].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cb61e558-cc2d-4fe6-b842-7b3a6a1d0ff3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1 Speech to Speech Chatbot for Airplane flying school | LINK \\uf0be Performed Text Processing to create Question Answer pair Template \\uf0be Used these Tools and technologies Transformers, Llama-2-7b, NLP, PyTorch, Hugging Face, LoRA, QLoRA, Parameter efficient Fine Tuning(PEFT), Supervised Fine Tuning(SFT) \\uf0be Then Re-trained the Llama-2-7b model using hugging face API and dataset I took was, Airplane Flying Handbook of US Federal Aviation Administration(DOT-FAA). \\uf0be Took prompts in speech medium and then used Speech to text technology to convert speech into text and then passed the text to our customized chatbot. The chat gave response in text form which I converted it into speech medium using Text to Speech technology. . 2 Object detection model | LINK \\uf0be Used Transfer Learning to get pretrained YOLOv8 model from huggingface. \\uf0be Improvements done: 1. Added audio to the output which is not present in output of yolov8. 2. Calculated distance between objects in the video frame. 3. Created circular bounding box instead of rectangular bounding box. KUNAL KUMAR DATA SCIENTIST Phone No: 9060804108 Email: kunal.jcdu@gmail.com Linkedin | ( Kunal Kumar) Github | (SolveKunal) SKILLS SUMMARY WORK EXPERIENCE DATA ANALYST INTERN | LINK \\uf0b7 Streamlined Data collection and reporting procedures, reducing processing time by 10%, enhancing efficiency. \\uf0b7 Collaborated with 3+ cross-functional teams to gather information to decide Project scopes and ensure alignment with business objectives. \\uf0b7 Conducted in-depth market research and analysis resulting identification of 5+ key trends and created Dashboard using power BI PROJECTS Languages: Python, SQL Frameworks: Matplotlib, Seaborn, NLTK, Pandas, Numpy, Sicklet-Learn, Tools: Power BI, Tableau, MySQL, Azure, Visual studio code, Jupyter Notebook, . . Speech to Text, Text to Speech, pyttsx3, Speech Recognizer Technical Skills: NLP, LLMs, GenAI solution using Azure OpenAI, Computer Vision, Object detection, Image Classification, similar sentence search, article summarizer, Machine Learning, supervised Learning, Linear Regression , Logistic Regression, Random Forest, Decision Tree, Data Modelling, Data Cleaning, Exploratory Data Analysis, Data Visualization, Predictive Analytics, Market Analysis, Sentiment Analysis, Semantic Search, Transfer Learning, TensorFlow, Keras, Trend Analysis 3 Pneumonia Detection LINK \\uf0be Built a CNN network to classify chest X-ray images as pneumonia-affected or normal. \\uf0be Achieved 75.66% accuracy. Plan to improve accuracy by optimizing image size and model parameters. \\uf0be Used these Tools and Technology: CNN, Deep Learning, Transfer Learning, Keras Tensorflow, VGG-16 4 Live Dashboard creation \\uf0be Performed Data Cleaning and Exploratory data Analysis in python and then transferred the cleaned dataset to Power BI. \\uf0be Identified 10+ major trends from the dataset which contributed to decision-making process. \\uf0be Interactive Dashboard was built and the link was sent to the University Professor. 5 Sentiment Analysis of 50000 Movie Reviews and Similar Sentence Search \\uf0be Performed Text Processing and extracted Embeddings for each word using Word2Vec and Transformers model. \\uf0be Then trained a Random-Forest Classifier with Embeddings as inputs to predict Sentiment of each movie review and do Similar Sentence Search in a supervised technique mode. \\uf0be Using Transformer model I achieved more similar sentence searches as here Context Aware Embeddings were used. 6 Restaurant Revenue Prediction \\uf0be This project was part of the hackathon organized by Great Learning platform. \\uf0be Performed Data Cleaning and Exploratory Data Analysis and then built a Random Forest Regressor ML model \\uf0be I secured 55 Rank in this Hackathon, with 85.5% accuracy. \\uf0be However, after the competition was over, I increased the models accuracy to 95.5% using XG-Boost model. \\uf0d8 Post Graduation in Artificial Intelligence and Machine Learning Undergoing University of Texas at Austin \\uf0d8 B-Tech in Electronics and Communication Engineering 2014-2018 Vellore Institute of Technology (VIT Vellore) EDUCATION'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extracted_name3=extract_name3(preprocessed_text)\n",
    "extracted_name3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "87c19247-88b7-4bfc-b1a0-05eaa2eee132",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_email(text):\n",
    "    email_pattern = r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b'\n",
    "    email = re.findall(email_pattern, text)\n",
    "    return email"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6ab04e9e-d10c-4d39-908a-47b2bd7f76d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['kunal.jcdu@gmail.com']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "email=extract_email(extracted_name3)\n",
    "email"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a756b34-94d2-45bf-812d-23792abba615",
   "metadata": {},
   "source": [
    "# THUS EMAIL HAS BEEN EXTRACTED ACCURATELY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4ea53c3f-fcce-4611-ab79-b2cf7a64001d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_phone_number(text):\n",
    "    phone_pattern = r'\\b\\d{10}\\b'\n",
    "    phone = re.findall(phone_pattern, text)\n",
    "    return phone[0] if phone else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "af68716e-9bb7-4895-ad13-5ac53cbc9de5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'9060804108'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phone_number=extract_phone_number(extracted_name3)\n",
    "phone_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e1d215d5-3b19-4095-aab5-43900ae8c1b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_skills(text):\n",
    "    skills_pattern = re.compile(r'Skills(.*?)(Experience|Education|Projects|$)', re.DOTALL)\n",
    "    skills = skills_pattern.findall(text)\n",
    "    if skills:\n",
    "        skills_list = skills[0][0].strip().split(',')\n",
    "        return [skill.strip() for skill in skills_list]\n",
    "    return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "19b36c18-f897-4ea7-94e5-0ae1f888b46c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[': NLP',\n",
       " 'LLMs',\n",
       " 'GenAI solution using Azure OpenAI',\n",
       " 'Computer Vision',\n",
       " 'Object detection',\n",
       " 'Image Classification',\n",
       " 'similar sentence search',\n",
       " 'article summarizer',\n",
       " 'Machine Learning',\n",
       " 'supervised Learning',\n",
       " 'Linear Regression',\n",
       " 'Logistic Regression',\n",
       " 'Random Forest',\n",
       " 'Decision Tree',\n",
       " 'Data Modelling',\n",
       " 'Data Cleaning',\n",
       " 'Exploratory Data Analysis',\n",
       " 'Data Visualization',\n",
       " 'Predictive Analytics',\n",
       " 'Market Analysis',\n",
       " 'Sentiment Analysis',\n",
       " 'Semantic Search',\n",
       " 'Transfer Learning',\n",
       " 'TensorFlow',\n",
       " 'Keras',\n",
       " 'Trend Analysis 3 Pneumonia Detection LINK \\uf0be Built a CNN network to classify chest X-ray images as pneumonia-affected or normal. \\uf0be Achieved 75.66% accuracy. Plan to improve accuracy by optimizing image size and model parameters. \\uf0be Used these Tools and Technology: CNN',\n",
       " 'Deep Learning',\n",
       " 'Transfer Learning',\n",
       " 'Keras Tensorflow',\n",
       " 'VGG-16 4 Live Dashboard creation \\uf0be Performed Data Cleaning and Exploratory data Analysis in python and then transferred the cleaned dataset to Power BI. \\uf0be Identified 10+ major trends from the dataset which contributed to decision-making process. \\uf0be Interactive Dashboard was built and the link was sent to the University Professor. 5 Sentiment Analysis of 50000 Movie Reviews and Similar Sentence Search \\uf0be Performed Text Processing and extracted Embeddings for each word using Word2Vec and Transformers model. \\uf0be Then trained a Random-Forest Classifier with Embeddings as inputs to predict Sentiment of each movie review and do Similar Sentence Search in a supervised technique mode. \\uf0be Using Transformer model I achieved more similar sentence searches as here Context Aware Embeddings were used. 6 Restaurant Revenue Prediction \\uf0be This project was part of the hackathon organized by Great Learning platform. \\uf0be Performed Data Cleaning and Exploratory Data Analysis and then built a Random Forest Regressor ML model \\uf0be I secured 55 Rank in this Hackathon',\n",
       " 'with 85.5% accuracy. \\uf0be However',\n",
       " 'after the competition was over',\n",
       " 'I increased the models accuracy to 95.5% using XG-Boost model. \\uf0d8 Post Graduation in Artificial Intelligence and Machine Learning Undergoing University of Texas at Austin \\uf0d8 B-Tech in Electronics and Communication Engineering 2014-2018 Vellore Institute of Technology (VIT Vellore) EDUCATION']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skills=extract_skills(extracted_name3)\n",
    "skills"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f7789307-0c8f-4148-90e8-b354d50fbb56",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(': NLP, LLMs, GenAI solution using Azure OpenAI, Computer Vision, Object detection, Image Classification, similar sentence search, article summarizer, Machine Learning, supervised Learning, Linear Regression , Logistic Regression, Random Forest, Decision Tree, Data Modelling, Data Cleaning, Exploratory Data Analysis, Data Visualization, Predictive Analytics, Market Analysis, Sentiment Analysis, Semantic Search, Transfer Learning, TensorFlow, Keras, Trend Analysis 3 Pneumonia Detection LINK \\uf0be Built a CNN network to classify chest X-ray images as pneumonia-affected or normal. \\uf0be Achieved 75.66% accuracy. Plan to improve accuracy by optimizing image size and model parameters. \\uf0be Used these Tools and Technology: CNN, Deep Learning, Transfer Learning, Keras Tensorflow, VGG-16 4 Live Dashboard creation \\uf0be Performed Data Cleaning and Exploratory data Analysis in python and then transferred the cleaned dataset to Power BI. \\uf0be Identified 10+ major trends from the dataset which contributed to decision-making process. \\uf0be Interactive Dashboard was built and the link was sent to the University Professor. 5 Sentiment Analysis of 50000 Movie Reviews and Similar Sentence Search \\uf0be Performed Text Processing and extracted Embeddings for each word using Word2Vec and Transformers model. \\uf0be Then trained a Random-Forest Classifier with Embeddings as inputs to predict Sentiment of each movie review and do Similar Sentence Search in a supervised technique mode. \\uf0be Using Transformer model I achieved more similar sentence searches as here Context Aware Embeddings were used. 6 Restaurant Revenue Prediction \\uf0be This project was part of the hackathon organized by Great Learning platform. \\uf0be Performed Data Cleaning and Exploratory Data Analysis and then built a Random Forest Regressor ML model \\uf0be I secured 55 Rank in this Hackathon, with 85.5% accuracy. \\uf0be However, after the competition was over, I increased the models accuracy to 95.5% using XG-Boost model. \\uf0d8 Post Graduation in Artificial Intelligence and Machine Learning Undergoing University of Texas at Austin \\uf0d8 B-Tech in Electronics and Communication Engineering 2014-2018 Vellore Institute of Technology (VIT Vellore) EDUCATION ',\n",
       "  '')]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#def extract_skills(text):\n",
    "skills_pattern = r'Skills(.*?)(Experience|Education|Projects|$)'\n",
    "skills = re.findall(skills_pattern,preprocessed_text)\n",
    "skills"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a0980a81-54d1-484f-8250-09d9f1b9d0ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "': NLP, LLMs, GenAI solution using Azure OpenAI, Computer Vision, Object detection, Image Classification, similar sentence search, article summarizer, Machine Learning, supervised Learning, Linear Regression , Logistic Regression, Random Forest, Decision Tree, Data Modelling, Data Cleaning, Exploratory Data Analysis, Data Visualization, Predictive Analytics, Market Analysis, Sentiment Analysis, Semantic Search, Transfer Learning, TensorFlow, Keras, Trend Analysis 3 Pneumonia Detection LINK \\uf0be Built a CNN network to classify chest X-ray images as pneumonia-affected or normal. \\uf0be Achieved 75.66% accuracy. Plan to improve accuracy by optimizing image size and model parameters. \\uf0be Used these Tools and Technology: CNN, Deep Learning, Transfer Learning, Keras Tensorflow, VGG-16 4 Live Dashboard creation \\uf0be Performed Data Cleaning and Exploratory data Analysis in python and then transferred the cleaned dataset to Power BI. \\uf0be Identified 10+ major trends from the dataset which contributed to decision-making process. \\uf0be Interactive Dashboard was built and the link was sent to the University Professor. 5 Sentiment Analysis of 50000 Movie Reviews and Similar Sentence Search \\uf0be Performed Text Processing and extracted Embeddings for each word using Word2Vec and Transformers model. \\uf0be Then trained a Random-Forest Classifier with Embeddings as inputs to predict Sentiment of each movie review and do Similar Sentence Search in a supervised technique mode. \\uf0be Using Transformer model I achieved more similar sentence searches as here Context Aware Embeddings were used. 6 Restaurant Revenue Prediction \\uf0be This project was part of the hackathon organized by Great Learning platform. \\uf0be Performed Data Cleaning and Exploratory Data Analysis and then built a Random Forest Regressor ML model \\uf0be I secured 55 Rank in this Hackathon, with 85.5% accuracy. \\uf0be However, after the competition was over, I increased the models accuracy to 95.5% using XG-Boost model. \\uf0d8 Post Graduation in Artificial Intelligence and Machine Learning Undergoing University of Texas at Austin \\uf0d8 B-Tech in Electronics and Communication Engineering 2014-2018 Vellore Institute of Technology (VIT Vellore) EDUCATION '"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=skills[0][0]\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "aac92645-8a0d-44c7-bbe0-dd92fa578460",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "': NLP, LLMs, GenAI solution using Azure OpenAI, Computer Vision, Object detection, Image Classification, similar sentence search, article summarizer, Machine Learning, supervised Learning, Linear Regression , Logistic Regression, Random Forest, Decision Tree, Data Modelling, Data Cleaning, Exploratory Data Analysis, Data Visualization, Predictive Analytics, Market Analysis, Sentiment Analysis, Semantic Search, Transfer Learning, TensorFlow, Keras, Trend Analysis 3 Pneumonia Detection LINK \\uf0be Built a CNN network to classify chest X-ray images as pneumonia-affected or normal. \\uf0be Achieved 75.66% accuracy. Plan to improve accuracy by optimizing image size and model parameters. \\uf0be Used these Tools and Technology: CNN, Deep Learning, Transfer Learning, Keras Tensorflow, VGG-16 4 Live Dashboard creation \\uf0be Performed Data Cleaning and Exploratory data Analysis in python and then transferred the cleaned dataset to Power BI. \\uf0be Identified 10+ major trends from the dataset which contributed to decision-making process. \\uf0be Interactive Dashboard was built and the link was sent to the University Professor. 5 Sentiment Analysis of 50000 Movie Reviews and Similar Sentence Search \\uf0be Performed Text Processing and extracted Embeddings for each word using Word2Vec and Transformers model. \\uf0be Then trained a Random-Forest Classifier with Embeddings as inputs to predict Sentiment of each movie review and do Similar Sentence Search in a supervised technique mode. \\uf0be Using Transformer model I achieved more similar sentence searches as here Context Aware Embeddings were used. 6 Restaurant Revenue Prediction \\uf0be This project was part of the hackathon organized by Great Learning platform. \\uf0be Performed Data Cleaning and Exploratory Data Analysis and then built a Random Forest Regressor ML model \\uf0be I secured 55 Rank in this Hackathon, with 85.5% accuracy. \\uf0be However, after the competition was over, I increased the models accuracy to 95.5% using XG-Boost model. \\uf0d8 Post Graduation in Artificial Intelligence and Machine Learning Undergoing University of Texas at Austin \\uf0d8 B-Tech in Electronics and Communication Engineering 2014-2018 Vellore Institute of Technology (VIT Vellore) EDUCATION'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b=a.strip()\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "75b11cb9-2c8e-4d9e-8923-73dedd4ea80d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[': NLP',\n",
       " ' LLMs',\n",
       " ' GenAI solution using Azure OpenAI',\n",
       " ' Computer Vision',\n",
       " ' Object detection',\n",
       " ' Image Classification',\n",
       " ' similar sentence search',\n",
       " ' article summarizer',\n",
       " ' Machine Learning',\n",
       " ' supervised Learning',\n",
       " ' Linear Regression ',\n",
       " ' Logistic Regression',\n",
       " ' Random Forest',\n",
       " ' Decision Tree',\n",
       " ' Data Modelling',\n",
       " ' Data Cleaning',\n",
       " ' Exploratory Data Analysis',\n",
       " ' Data Visualization',\n",
       " ' Predictive Analytics',\n",
       " ' Market Analysis',\n",
       " ' Sentiment Analysis',\n",
       " ' Semantic Search',\n",
       " ' Transfer Learning',\n",
       " ' TensorFlow',\n",
       " ' Keras',\n",
       " ' Trend Analysis 3 Pneumonia Detection LINK \\uf0be Built a CNN network to classify chest X-ray images as pneumonia-affected or normal. \\uf0be Achieved 75.66% accuracy. Plan to improve accuracy by optimizing image size and model parameters. \\uf0be Used these Tools and Technology: CNN',\n",
       " ' Deep Learning',\n",
       " ' Transfer Learning',\n",
       " ' Keras Tensorflow',\n",
       " ' VGG-16 4 Live Dashboard creation \\uf0be Performed Data Cleaning and Exploratory data Analysis in python and then transferred the cleaned dataset to Power BI. \\uf0be Identified 10+ major trends from the dataset which contributed to decision-making process. \\uf0be Interactive Dashboard was built and the link was sent to the University Professor. 5 Sentiment Analysis of 50000 Movie Reviews and Similar Sentence Search \\uf0be Performed Text Processing and extracted Embeddings for each word using Word2Vec and Transformers model. \\uf0be Then trained a Random-Forest Classifier with Embeddings as inputs to predict Sentiment of each movie review and do Similar Sentence Search in a supervised technique mode. \\uf0be Using Transformer model I achieved more similar sentence searches as here Context Aware Embeddings were used. 6 Restaurant Revenue Prediction \\uf0be This project was part of the hackathon organized by Great Learning platform. \\uf0be Performed Data Cleaning and Exploratory Data Analysis and then built a Random Forest Regressor ML model \\uf0be I secured 55 Rank in this Hackathon',\n",
       " ' with 85.5% accuracy. \\uf0be However',\n",
       " ' after the competition was over',\n",
       " ' I increased the models accuracy to 95.5% using XG-Boost model. \\uf0d8 Post Graduation in Artificial Intelligence and Machine Learning Undergoing University of Texas at Austin \\uf0d8 B-Tech in Electronics and Communication Engineering 2014-2018 Vellore Institute of Technology (VIT Vellore) EDUCATION']"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c=b.split(',')\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fdd0ac4-e851-4f93-830b-d7b1207732b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_form(url, name, email, phone_number, skills):\n",
    "    form_data = {\n",
    "        'name': name,\n",
    "        'email': email,\n",
    "        'phone_number': phone_number,\n",
    "        'skills': ', '.join(skills)\n",
    "    }\n",
    "    response = requests.post(url, data=form_data)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e4cc676-55bd-4c8e-a2d3-0dc699d4fb45",
   "metadata": {},
   "outputs": [],
   "source": [
    "form_url = \"http://example.com/submit_form\"\n",
    "response = fill_form(form_url, name, email, phone_number, skills)\n",
    "print(response.status_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e27474a2-bc6e-4e61-bf7c-3c1ffc26b01e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read and preprocess the resume\n",
    "resume_text = read_pdf(\"path_to_resume.pdf\")\n",
    "cleaned_text = preprocess_text(resume_text)\n",
    "\n",
    "# Extract information\n",
    "name = extract_name(cleaned_text)\n",
    "email = extract_email(cleaned_text)\n",
    "phone_number = extract_phone_number(cleaned_text)\n",
    "skills = extract_skills(cleaned_text)\n",
    "\n",
    "# Fill out and submit the form\n",
    "form_url = \"http://example.com/submit_form\"\n",
    "response = fill_form(form_url, name, email, phone_number, skills)\n",
    "print(response.status_code)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
